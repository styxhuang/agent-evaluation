
# Agent 评估体系 - 对话整理与第一级落地方案（工具调用一并评估）

## 1. 目标（我们最终想解决什么）
- 要做一套 agent 评估体系，且 agent 调用的工具也要纳入评估。
- 网上常见说法是三层评估；当前聚焦第一级：代码直接调用层。

## 2. 已确认的约束与优先级（来自本次对话）
- L1 允许真实访问工具（允许联网调用 Bohrium MCP SSE）。
- 优先评估 agent 的工具调用策略稳定性；工具质量次之（因为未来工具会更多）。
- 评分指标尚未确定，需要一个可迭代的评分框架。
- 先本地运行，后续迁移到 CI。

## 3. 三层评估的“一级”定义（本项目采用的解释）
### 3.1 L1：代码直接调用（不评语言能力，评执行与策略工程质量）
核心思想：把“不稳定的 LLM 推理”和“可工程化的工具调用/执行”拆开。

- 工具层（Tool Quality, 次优先）：
  - 直接通过代码调用工具适配层（例如 MCP client），验证工具契约、稳定性、延迟与基础正确性。
- 策略层（Tool-Use Policy, 优先）：
  - 不要求 agent 当场“生成” tool call；而是用固定的 tool call 序列/日志回放/规则生成的候选调用，来评估：
    - 选工具是否正确
    - 参数是否合规
    - 是否满足预算
    - 失败时是否能按策略恢复（重试/降级/换工具/停止）

### 3.2 L2：带推理的离线评估（后续）
- 引入 LLM，让 agent 从自然语言任务中生成 tool calls，再执行与评分。
- 更像“端到端行为评估”，但会受到模型随机性与提示词敏感性影响。

### 3.3 L3：人工场景评估（后续）
- 真实流量、真实分布；评估长期稳定性、鲁棒性与成本。

## 4. 当前仓库现状（可复用资产）
- 已具备直接通过 SSE 调用 MCP 工具的脚本：
  - demo.py：直接 list_tools + call_tool
  - demo2.py：可打印工具 input schema

这说明“L1 工具直接调用”的最小路径已打通，下一步是把“手工探索脚本”升级为“可重复、可统计、可定位”的评估 runner。

## 5. L1 的实现骨架（推荐工程形态）
### 5.1 被测对象（SUT）
- Tool Adapter：统一接口 call(tool_name, args) -> result（背后用 MCP ClientSession）。
- Policy Checker（优先）：对一次 tool call 或 tool call 序列进行静态/规则校验。

### 5.2 测试用例（Case）结构（建议字段）
- case_id：唯一标识
- scenario：场景描述（用于分组/报告，不用于模型推理）
- tool_calls：一个序列（支持多步），每步包含：tool_name + args + budget
- oracle：判定器（schema/关键字段/阈值/失败预期）
- tags：smoke/regression/slow 等

### 5.3 运行模式
- Live：真实调用工具（你已确认允许）
- Record（建议尽早引入）：保存归一化后的 result，便于回归
- Replay（建议用于 CI）：不依赖外网与工具波动，结果稳定

## 6. 评分框架（先可用、再校准）
你当前“不确定如何评分”是正常的：评分不是拍脑袋定数值，而是先做可观测、可分解、可定位的指标体系，然后用数据校准权重。

### 6.1 评分对象拆分（保证策略优先）
L1 总分建议拆成两块：
- S_policy：策略与合规（优先）
- S_tool：工具执行质量（次优先）

总分示例：S_total = 0.7 * S_policy + 0.3 * S_tool

### 6.2 S_policy（策略与合规）推荐维度
把“策略稳定性”具体化为可测项：

1) 工具选择正确率（Tool Selection）
- 在给定场景/步骤上，是否选了允许集合内的工具
- 是否避免明显不相关工具

2) 参数合规率（Argument Validity）
- 必填字段齐全、类型正确、枚举/范围满足 schema
- 预算约束满足（例如 n_results 不超过上限）

3) 调用序列合理性（Call Order / Plan Shape）
- 多步任务的调用顺序是否满足前置依赖
- 不做无意义重复调用（可用“重复率/循环检测”做扣分）

4) 失败恢复质量（Recovery）
- 超时/5xx/空结果时：是否按策略重试（次数上限）
- 是否有降级路径（减少 n_results、换查询条件、停止并上报）

S_policy 计算建议：
- 以 step 为粒度统计，每项 0/1 或 0~1 分；对“严重违规”设硬性扣分（如参数缺必填、越权/超预算）。

### 6.3 S_tool（工具执行质量）推荐维度
1) 成功率（Success Rate）
- call_tool 无异常且返回可解析结果

2) 契约一致性（Contract）
- 输出可被解析为预期格式（JSON/text），关键字段存在

3) 性能（Latency）
- 记录 P50/P95；超过阈值扣分（阈值先用经验值，之后用历史分布校准）

### 6.4 “先跑起来”的权重模板（可迭代）
为避免一开始争论权重，建议先用一个可解释的默认模板：

- S_policy（70 分）
  - 工具选择：25
  - 参数合规：25
  - 调用序列：10
  - 失败恢复：10
- S_tool（30 分）
  - 成功率：15
  - 契约一致性：10
  - 延迟：5

## 7. 本地到 CI 的迁移路线
### 7.1 本地阶段（第一里程碑）
- 目标：可重复跑一组 case，产出机器可读报告（json）+ 人可读摘要。
- 最小集合：
  - 1 个工具：fetch_bohrium_crystals
  - 3 类用例：正例/边界例/反例（参数缺失或超预算）
  - 统计：通过率、失败原因分类、P95 延迟

### 7.2 CI 阶段（第二里程碑）
- 默认跑 Replay（稳定、快、无外部依赖）
- 定期（nightly）跑 Live（监控工具质量与漂移）

## 8. 下一步需要补齐的数据缺口（Data Gaps）
为了把策略评估做扎实，L1 还缺三类输入：
- 允许的工具集合与“场景 -> 工具”的粗映射（哪类任务该用哪个工具）
- 每个工具的参数预算规则（上限/默认值/危险参数）
- 失败恢复策略规范（重试次数、降级顺序、停止条件）

可以先从 1 个工具开始，把这些规则写成一个最小“政策”文件，然后逐步扩展到更多工具。
